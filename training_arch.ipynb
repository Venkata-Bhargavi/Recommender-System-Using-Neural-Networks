{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87dd1934-32d3-4fb7-b25f-f2851dfa3d02",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5ec14e4-40a0-4eed-b528-0c16b8fbd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import sklearn\n",
    "import random\n",
    "import zipfile\n",
    "import torch\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from zipfile import ZipFile\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a167f9c8-1949-4881-a43d-6e9bfc38d05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a71f83-646e-455a-beab-b45279fb7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieUserRatingDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 rated_users: pd.DataFrame,\n",
    "                 total_samples: pd.DataFrame,\n",
    "                 unrated_user_ratio: int\n",
    "                 ) -> None:\n",
    "        super(MovieUserRatingDataset, self).__init__()\n",
    "\n",
    "        self.rated_users = rated_users\n",
    "        self.total_samples = total_samples\n",
    "        self.unrated_user_ratio = unrated_user_ratio\n",
    "\n",
    "        self.users, self.items, self.ratings = self.unrated_user_sampling()\n",
    "\n",
    "    \n",
    "    def unrated_user_sampling(self):\n",
    "        rated_users = self.rated_users\n",
    "        total_samples = self.total_samples\n",
    "        users_list, items_list, labels_list = [], [], []\n",
    "        user_item_set = set(zip(rated_users['userId'], rated_users['movieId']))\n",
    "        total_user_item_set = set(zip(total_samples['userId'], total_samples['movieId']))\n",
    "        all_movie_ids = total_samples['movieId'].unique()\n",
    "        unrated_user_ratio = self.unrated_user_ratio\n",
    "\n",
    "        for user, item in user_item_set:\n",
    "            # Add positive instance\n",
    "            users_list.append(user)\n",
    "            items_list.append(item)\n",
    "            labels_list.append(1.0)\n",
    "\n",
    "            # Initialize visited items\n",
    "            rated = [item]\n",
    "\n",
    "            # Add negative instances\n",
    "            for _ in range(unrated_user_ratio):\n",
    "                # Randomly select a negative item\n",
    "                negative_item = np.random.choice(all_movie_ids)\n",
    "\n",
    "                # Ensure the negative item is not in the total dataset and has not been visited\n",
    "                while (user, negative_item) in total_user_item_set or negative_item in rated:\n",
    "                    negative_item = np.random.choice(all_movie_ids)\n",
    "\n",
    "                users_list.append(user)\n",
    "                items_list.append(negative_item)\n",
    "                rated.append(negative_item)\n",
    "                labels_list.append(0.0)\n",
    "\n",
    "        print(f\"Not rated user sampled data size: {len(labels_list)}\")\n",
    "        return torch.tensor(users_list), torch.tensor(items_list), torch.tensor(labels_list)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.users[index], self.items[index], self.ratings[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce294415-b17a-4eba-bfbf-de0b68369835",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (A Custom Model for Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae96a70-750f-45ce-af30-ab5f80aa329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedModelLoader(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PretrainedModelLoader, self).__init__()\n",
    "\n",
    "    def load_pretrained_model(self, model, pretrained_model):\n",
    "        model.user_embedding.weight.data.copy_(pretrained_model.user_embedding.weight)\n",
    "        model.movie_item_embedding.weight.data.copy_(pretrained_model.movie_item_embedding.weight)\n",
    "        for layer, pretrained_layer in zip(model.multilayer_model, pretrained_model.multilayer_model):\n",
    "            if isinstance(layer, nn.Linear) and isinstance(pretrained_layer, nn.Linear):\n",
    "                layer.weight.data.copy_(pretrained_layer.weight)\n",
    "                layer.bias.data.copy_(pretrained_layer.bias)\n",
    "\n",
    "class UserRatingsMultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self,num_users:int,num_items:int,num_factor:int=8,total_hidden_layers=None,use_pretrained: bool = False,NeuralMF:bool = False,pretrained_model=None\n",
    "                 ):\n",
    "        super(UserRatingsMultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        if total_hidden_layers is None:\n",
    "            total_hidden_layers = [64,32,16]\n",
    "\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.user_embedding = nn.Embedding(num_users, total_hidden_layers[0] // 2)\n",
    "        self.movie_item_embedding = nn.Embedding(num_items, total_hidden_layers[0] // 2)\n",
    "        self.NeuralMF = NeuralMF\n",
    "        multiple_hidden_layers = []\n",
    "\n",
    "        for idx, layer_size in enumerate(total_hidden_layers):\n",
    "            # Add layers to the MLP model\n",
    "            multiple_hidden_layers.append(nn.Linear(layer_size, layer_size // 2))\n",
    "            multiple_hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        # stack the layers\n",
    "        self.multilayer_model = nn.Sequential(*multiple_hidden_layers)\n",
    "\n",
    "        # initialize prediction layer\n",
    "        self.predict_layer = nn.Linear(total_hidden_layers[-1] // 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        if self.use_pretrained:\n",
    "            PretrainedModelLoader().load_pretrained_model(self, self.pretrained_model)\n",
    "        else:\n",
    "            self._init_weights()\n",
    "            \n",
    "    # Initialize weights\n",
    "    def _init_weights(self):\n",
    "        if not self.use_pretrained:\n",
    "            nn.init.normal_(self.user_embedding.weight, std=1e-2)\n",
    "            nn.init.normal_(self.movie_item_embedding.weight, std=1e-2)\n",
    "            for layer in self.multilayer_model:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "        if not self.NeuralMF:\n",
    "            nn.init.normal_(self.predict_layer.weight, std=1e-2)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        '''\n",
    "        function for forward propogation to generate aggregated signal\n",
    "        '''\n",
    "        # Convert user tensor to LongTensor\n",
    "        user = user.long()\n",
    "        item = item.long()\n",
    "\n",
    "        # Embeding movie and user\n",
    "        embedding_user = self.user_embedding(user)\n",
    "        embedding_item = self.movie_item_embedding(item)\n",
    "\n",
    "        # combining (concatenating) users and movie(item embeddings)\n",
    "        concatinating_embed_input = torch.cat((embedding_user, embedding_item), dim=-1)\n",
    "\n",
    "        # Forward pass through MLP layers\n",
    "        aggregated_signal = self.multilayer_model(concatinating_embed_input)\n",
    "\n",
    "        if not self.NeuralMF:\n",
    "            # Forward Propogation with sigmoid activation function for classification\n",
    "            aggregated_signal = self.predict_layer(aggregated_signal)\n",
    "            aggregated_signal = self.sigmoid(aggregated_signal)\n",
    "            aggregated_signal = aggregated_signal.view(-1)\n",
    "\n",
    "        return aggregated_signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d6cd9-1be6-462e-8c61-f89e0a387c03",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79bbfc27-cbb4-4626-b1ec-1da7166d255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_hit_rate(gt_item, pred_items):\n",
    "\n",
    "    if gt_item in pred_items:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def compute_ndcg(gt_item, pred_items):\n",
    "\n",
    "    if gt_item in pred_items:\n",
    "        index = pred_items.index(gt_item)\n",
    "        return np.reciprocal(np.log2(index+2))\n",
    "    return 0\n",
    "\n",
    "def compute_metrics(model, test_loader, top_k, device):\n",
    "\n",
    "    hit_rate, ndcg_score = [], []\n",
    "\n",
    "    for user, item, label in test_loader:\n",
    "        \n",
    "        user = user.to(device) #moving user or item to device(CPU or GPU)\n",
    "        item = item.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "\n",
    "        # Get recommended items\n",
    "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
    "\n",
    "        # Computing hit rate and cumulative gain (NDCG) for each user\n",
    "        gt_item = item[0].item()\n",
    "        hit_rate.append(compute_hit_rate(gt_item, recommends))\n",
    "        ndcg_score.append(compute_ndcg(gt_item, recommends))\n",
    "\n",
    "    # Compute mean HR and NDCG\n",
    "    hit_rate_mean = np.mean(hit_rate)\n",
    "    mean_ndcg = np.mean(ndcg_score)\n",
    "\n",
    "    return hit_rate_mean, mean_ndcg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ef9d8-1e45-4780-a38c-22ec10ebd186",
   "metadata": {},
   "source": [
    "## Training-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b23d59e2-c9ec-4ee0-aaae-f9cf0033a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class MLP_Trainer():\n",
    "    def __init__(self, model, optimizer, epochs, dataloader, criterion, test_obj, device='cuda', print_cost=True):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.dataloader = dataloader\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.print_cost = print_cost\n",
    "        self.test_obj = test_obj\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "        total_epochs = self.epochs\n",
    "        dataloader = self.dataloader\n",
    "        criterion = self.criterion\n",
    "        total_batch = len(dataloader)\n",
    "        device = self.device\n",
    "        test_obj = self.test_obj\n",
    "\n",
    "        losses_list = []\n",
    "        #iterating over epochs\n",
    "        for epoch in range(total_epochs):\n",
    "            for user, item, target in dataloader:\n",
    "                user, item, target = user.to(device), item.to(device), target.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(user, item)\n",
    "                loss = criterion(pred, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if self.print_cost:\n",
    "                hit_rate, NDCG_score = compute_metrics(model, test_obj, 10, device)\n",
    "                print(\"Epoch: {} Hit_Rate: {:.3f}\\tNDCG: {:.3f}\".format(epoch, np.mean(hit_rate), np.mean(NDCG_score)))\n",
    "\n",
    "            losses_list.append(loss.item())\n",
    "\n",
    "        if self.print_cost:\n",
    "            print('--------------Training Completed-----------')\n",
    "        #returning list of training losses\n",
    "        return losses_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce693d2-c5c3-40a6-b40e-ba8804ec517e",
   "metadata": {},
   "source": [
    "## Checking GPU/CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec99e7b4-9fd0-447e-9d70-d2d81be61f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Print GPU information\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device:', torch.cuda.current_device())\n",
    "    print('No of GPUs in use', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94030a20-2d4c-4f0b-b611-bd214976100f",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464d971c-18da-42d4-ac3c-2cbb87b5c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to save checkpoints (model)\n",
    "pretrain_dir = 'models_trained'\n",
    "if not os.path.isdir(pretrain_dir):\n",
    "    os.makedirs(pretrain_dir)\n",
    "\n",
    "# Load train, test, and total datasets\n",
    "train_dataframe = pd.read_csv(\"/home/sikhakolli.v/rec/data/interim/train.csv\")\n",
    "total_dataframe = pd.read_csv(\"/home/sikhakolli.v/rec/data/interim/entire_dataset.csv\")\n",
    "test_dataframe = pd.read_csv(\"/home/sikhakolli.v/rec/data/interim/evaluation.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de16140-7c9a-40fe-84b8-83079fb0613a",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c10703-987e-4a67-81e6-e6a070e6c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not rated user sampled data size: 501130\n",
      "Not rated user sampled data size: 61000\n",
      "<class '__main__.MovieUserRatingDataset'>:<__main__.MovieUserRatingDataset object at 0x2b76380b6400>\n",
      "Data loaded successfully!\n",
      "UserRatingsMultiLayerPerceptron(\n",
      "  (user_embedding): Embedding(611, 64)\n",
      "  (movie_item_embedding): Embedding(193610, 64)\n",
      "  (multilayer_model): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (9): ReLU()\n",
      "  )\n",
      "  (predict_layer): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create MovieLens datasets\n",
    "train_set = MovieUserRatingDataset(rated_users=train_dataframe, total_samples=total_dataframe, unrated_user_ratio=4)\n",
    "test_set = MovieUserRatingDataset(rated_users=test_dataframe, total_samples=total_dataframe, unrated_user_ratio=99)\n",
    "\n",
    "print(f\"{type(train_set)}:{train_set}\")\n",
    "# Get number of unique users and movies\n",
    "max_num_users, max_num_items = total_dataframe['userId'].max() + 1, total_dataframe['movieId'].max() + 1\n",
    "print('Data loaded successfully!')\n",
    "\n",
    "# Data Loaders for effecient loading\n",
    "dataloader_train = DataLoader(dataset=train_set, batch_size=32, shuffle=True, num_workers=0)\n",
    "dataloader_test = DataLoader(dataset=test_set, batch_size=100, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "# Initializing model\n",
    "model = UserRatingsMultiLayerPerceptron(num_users=max_num_users, num_items=max_num_items, NeuralMF=False)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model.to(device)\n",
    "\n",
    "#Using Binary cross entropy loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f727f0db-982e-492a-8d2b-65cfd3f02d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: (tensor(89), tensor(3604), tensor(1.))\n",
      "Sample 2: (tensor(89), tensor(1282), tensor(0.))\n",
      "Sample 3: (tensor(89), tensor(53435), tensor(0.))\n",
      "Sample 4: (tensor(89), tensor(80363), tensor(0.))\n",
      "Sample 5: (tensor(89), tensor(214), tensor(0.))\n"
     ]
    }
   ],
   "source": [
    "#visualizing tensors\n",
    "for i in range(5):  # Printing first 5 samples\n",
    "    sample = train_set[i]\n",
    "    print(f\"Sample {i+1}: {sample}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887e7a82-1ca7-406e-87ef-592ffbeb461b",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f3b584-cd7c-4eb2-900f-db1f845dce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1                [-1, 1, 64]          39,104\n",
      "         Embedding-2                [-1, 1, 64]      12,391,040\n",
      "            Linear-3                [-1, 1, 64]           8,256\n",
      "              ReLU-4                [-1, 1, 64]               0\n",
      "            Linear-5                [-1, 1, 32]           2,080\n",
      "              ReLU-6                [-1, 1, 32]               0\n",
      "            Linear-7                [-1, 1, 16]             528\n",
      "              ReLU-8                [-1, 1, 16]               0\n",
      "            Linear-9                 [-1, 1, 8]             136\n",
      "             ReLU-10                 [-1, 1, 8]               0\n",
      "           Linear-11                 [-1, 1, 4]              36\n",
      "             ReLU-12                 [-1, 1, 4]               0\n",
      "           Linear-13                 [-1, 1, 1]               5\n",
      "          Sigmoid-14                 [-1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 12,441,185\n",
      "Trainable params: 12,441,185\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 47.46\n",
      "Estimated Total Size (MB): 47.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## layers: [128,64,32,16,8] \n",
    "    \n",
    "from torchsummary import summary  # Import torchsummary library\n",
    "# Move the model to the CPU\n",
    "model.cpu()\n",
    "\n",
    "# Move the input data to the CPU\n",
    "input_size_cpu = [(1,), (1,)]\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, input_size=input_size_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dacdd78f-4d9e-48f5-8ecf-207c902ca7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1                [-1, 1, 32]          19,552\n",
      "         Embedding-2                [-1, 1, 32]       6,195,520\n",
      "            Linear-3                [-1, 1, 32]           2,080\n",
      "              ReLU-4                [-1, 1, 32]               0\n",
      "            Linear-5                [-1, 1, 16]             528\n",
      "              ReLU-6                [-1, 1, 16]               0\n",
      "            Linear-7                 [-1, 1, 8]             136\n",
      "              ReLU-8                 [-1, 1, 8]               0\n",
      "            Linear-9                 [-1, 1, 4]              36\n",
      "             ReLU-10                 [-1, 1, 4]               0\n",
      "           Linear-11                 [-1, 1, 1]               5\n",
      "          Sigmoid-12                 [-1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 6,217,857\n",
      "Trainable params: 6,217,857\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 23.72\n",
      "Estimated Total Size (MB): 23.72\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## layers: [64,32,16,8] \n",
    "    \n",
    "from torchsummary import summary  # Import torchsummary library\n",
    "# Move the model to the CPU\n",
    "model.cpu()\n",
    "\n",
    "# Move the input data to the CPU\n",
    "input_size_cpu = [(1,), (1,)]\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, input_size=input_size_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57821977-bf32-4c99-9da2-4b6ab438cd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1                [-1, 1, 64]          39,104\n",
      "         Embedding-2                [-1, 1, 64]      12,391,040\n",
      "            Linear-3                [-1, 1, 64]           8,256\n",
      "              ReLU-4                [-1, 1, 64]               0\n",
      "            Linear-5                [-1, 1, 32]           2,080\n",
      "              ReLU-6                [-1, 1, 32]               0\n",
      "            Linear-7                [-1, 1, 16]             528\n",
      "              ReLU-8                [-1, 1, 16]               0\n",
      "            Linear-9                 [-1, 1, 8]             136\n",
      "             ReLU-10                 [-1, 1, 8]               0\n",
      "           Linear-11                 [-1, 1, 4]              36\n",
      "             ReLU-12                 [-1, 1, 4]               0\n",
      "           Linear-13                 [-1, 1, 1]               5\n",
      "          Sigmoid-14                 [-1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 12,441,185\n",
      "Trainable params: 12,441,185\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 47.46\n",
      "Estimated Total Size (MB): 47.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## layers: [64,32,16]\n",
    "    \n",
    "from torchsummary import summary  # Import torchsummary library\n",
    "# Move the model to the CPU\n",
    "model.cpu()\n",
    "\n",
    "# Move the input data to the CPU\n",
    "input_size_cpu = [(1,), (1,)]\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, input_size=input_size_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de2e68-3887-41ba-86f8-44f300b16c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
